---
layout: page
title: Reading Group
tagline: Paper Reading Sessions
permalink: /reading.html
ref: reading
order: 2
---

We meet on a weely basis to discuss recent research papers. Here is the schedule and the papers presented.

# Schedule

| Date | Speaker | Title |
| ----- | ------- | ----- |
| 04/08/21	| Angrosh |	VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification |
| 11/08/21	| Huda	| ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning |
| 18/08/21	| James	| Lookahead: A far-sighted alternative of magnitude-based pruning	|
| 25/08/21	| Micheal	| Prefix-Tuning: Optimizing Continuous Prompts for Generation	|
| 01/09/21	| Jodie	| Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations	|
| 08/09/21	| Danushka	| Is sparse attention interpretable	|
| 13/09/21	| Angrosh	| Dependency-driven RE with Attentive Graph Convolutional Networks	|
| 22/09/21	| Huda	| ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning	|
| 29/09/21	| James	| Does knowledge distillation really work?	|
| 06/10/21	| Micheal	| AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts	|
| 13/10/21	| Jodie	| MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models	|
| 20/10/21	| Danushka	| Dynamic contextualised word embeddings	|
| 27/10/21	| Huda	| Distilling Relation Embeddings from Pre-trained Language Models	|
| 3/11/21	| Angrosh	| Graph Transformer Networks	|	
| 17/11/21	| Micheal	| When does Further Pre-training MLM Help? An Empirical Study on Task-Oriented Dialog Pre-training	|
| 24/11/21	| Jodie	| ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension	|
| 19/01/22	| Danushka	| Knowlege base completion meets transfer learning	|
| 26/01/22	| Huda |	CoLAKE: Contextualized Language and Knowledge Embedding |
| 01/12/22	| Jodie	| |	
| 01/12/22	| Michael	| |


[Go to the Home Page]({{ '/' | absolute_url }})
